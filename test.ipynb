{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/u1/.conda/envs/Efficient/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py:507: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n",
      "WARNING:tensorflow:From /home/u1/.conda/envs/Efficient/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /ssd3/u1/NBI_NET/EfficientDet-master/layers.py:153: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (13 of 13) |########################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (13 of 13) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_fp=52.0, num_tp=13.0\n",
      "It cost 5.783306 sec\n",
      "13 instances of class cancer with average precision: 0.8797\n",
      "Max detections:  5\n",
      "IOU Threshold:  0.5\n",
      "mAP: 0.8797\n",
      "recall:  0.8757396449704141\n",
      "precison:  0.48030083092849124\n"
     ]
    }
   ],
   "source": [
    "from utils.compute_overlap import compute_overlap\n",
    "from utils.visualization import draw_detections, draw_annotations\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import progressbar\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "assert (callable(progressbar.progressbar)), \"Using wrong progressbar module, install 'progressbar2' instead.\"\n",
    "\n",
    "\n",
    "def _compute_ap(recall, precision):\n",
    "    \"\"\"\n",
    "    Compute the average precision, given the recall and precision curves.\n",
    "\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "\n",
    "    Args:\n",
    "        recall: The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "\n",
    "    Returns:\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "def _get_detections(generator, model, score_threshold=0.05, max_detections=1, visualize=False):\n",
    "    \"\"\"\n",
    "    Get the detections from the model using the generator.\n",
    "\n",
    "    The result is a list of lists such that the size is:\n",
    "        all_detections[num_images][num_classes] = detections[num_class_detections, 5]\n",
    "\n",
    "    Args:\n",
    "        generator: The generator used to run images through the model.\n",
    "        model: The model to run on the images.\n",
    "        score_threshold: The score confidence threshold to use.\n",
    "        max_detections: The maximum number of detections to use per image.\n",
    "        save_path: The path to save the images with visualized detections to.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists containing the detections for each image in the generator.\n",
    "\n",
    "    \"\"\"\n",
    "    all_detections = [[None for i in range(generator.num_classes()) if generator.has_label(i)] for j in\n",
    "                      range(generator.size())]\n",
    "\n",
    "    for i in progressbar.progressbar(range(generator.size()), prefix='Running network: '):\n",
    "        image = generator.load_image(i)\n",
    "        src_image = image.copy()\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        anchors = generator.anchors\n",
    "        image, scale = generator.preprocess_image(image)\n",
    "\n",
    "        # run network\n",
    "        boxes, scores, *_, labels = model.predict_on_batch([np.expand_dims(image, axis=0)])\n",
    "        boxes /= scale\n",
    "        boxes[:, :, 0] = np.clip(boxes[:, :, 0], 0, w - 1)\n",
    "        boxes[:, :, 1] = np.clip(boxes[:, :, 1], 0, h - 1)\n",
    "        boxes[:, :, 2] = np.clip(boxes[:, :, 2], 0, w - 1)\n",
    "        boxes[:, :, 3] = np.clip(boxes[:, :, 3], 0, h - 1)\n",
    "\n",
    "        # select indices which have a score above the threshold\n",
    "        indices = np.where(scores[0, :] > score_threshold)[0]\n",
    "\n",
    "        # select those scores\n",
    "        scores = scores[0][indices]\n",
    "\n",
    "        # find the order with which to sort the scores\n",
    "        scores_sort = np.argsort(-scores)[:max_detections]\n",
    "#         print(len(scores_sort))\n",
    "#         print(scores_sort)\n",
    "\n",
    "        # select detections\n",
    "        # (n, 4)\n",
    "        image_boxes = boxes[0, indices[scores_sort], :]\n",
    "        # (n, )\n",
    "        image_scores = scores[scores_sort]\n",
    "        # (n, )\n",
    "        image_labels = labels[0, indices[scores_sort]]\n",
    "        # (n, 6)\n",
    "        detections = np.concatenate(\n",
    "            [image_boxes, np.expand_dims(image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)\n",
    "#         print(detections)\n",
    "\n",
    "#         draw_annotations(src_image, generator.load_annotations(i), label_to_name=generator.label_to_name)\n",
    "#         draw_detections(src_image, detections[:5, :4], detections[:5, 4], detections[:5, 5].astype(np.int32),\n",
    "#                             label_to_name=generator.label_to_name,\n",
    "#                             score_threshold=score_threshold)\n",
    "\n",
    "#         # cv2.imwrite(os.path.join(save_path, '{}.png'.format(i)), raw_image)\n",
    "#         cv2.namedWindow('{}'.format(i), cv2.WINDOW_NORMAL)\n",
    "#         cv2.imshow('{}'.format(i), src_image)\n",
    "#         cv2.waitKey(0)\n",
    "\n",
    "        # copy detections to all_detections\n",
    "        for class_id in range(generator.num_classes()):\n",
    "            all_detections[i][class_id] = detections[detections[:, -1] == class_id, :-1]\n",
    "#         print(all_detections)\n",
    "        \n",
    "        f=open(\"/ssd3/u1/NBI_NET/EfficientDet-master/bbox.txt\",'a+')\n",
    "#         print(\"image_boxes= \",image_boxes)\n",
    "#         print(image_boxes.shape)\n",
    "        count=0\n",
    "        bbox_temp=[]\n",
    "        temp_single_position=0\n",
    "#         print(\"max_detections= \",max_detections)\n",
    "        temp_xmin,temp_ymin,temp_xmax,temp_ymax=[],[],[],[]\n",
    "        \n",
    "#         for i in range(len(scores_sort)):\n",
    "#             temp_xmin.append(image_boxes[i][0])\n",
    "#             temp_ymin.append(image_boxes[i][1])\n",
    "#             temp_xmax.append(image_boxes[i][2])\n",
    "#             temp_ymax.append(image_boxes[i][3])\n",
    "#         bbox_temp.append(min(temp_xmin))\n",
    "#         bbox_temp.append(min(temp_ymin))\n",
    "#         if max(temp_xmax)>480:\n",
    "#             bbox_temp.append(480)\n",
    "#         else:\n",
    "#             bbox_temp.append(max(temp_xmax))\n",
    "#         if max(temp_ymax)>480:\n",
    "#             bbox_temp.append(480)\n",
    "#         else:\n",
    "#             bbox_temp.append(max(temp_ymax))\n",
    "        \n",
    "        \n",
    "        for i in range(4):\n",
    "            for j in range(len(scores_sort)):                \n",
    "                temp_single_position=temp_single_position+image_boxes[j][count]\n",
    "            temp_single_position=temp_single_position/(max_detections)\n",
    "            if temp_single_position>480:\n",
    "                temp_single_position=480\n",
    "            bbox_temp.append(temp_single_position)\n",
    "            count+=1\n",
    "        bbox_temp=np.array(bbox_temp)\n",
    "#         print(bbox_temp.shape)\n",
    "#         print(\"image_boxes_append\",bbox_temp)\n",
    "        count=0\n",
    "        for i in range(4):\n",
    "            if count!=3:\n",
    "                f.write(str(bbox_temp[count])+',')\n",
    "            else:\n",
    "                f.write(str(bbox_temp[count]))\n",
    "            count+=1\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "    return all_detections\n",
    "\n",
    "\n",
    "def _get_annotations(generator):\n",
    "    \"\"\"\n",
    "    Get the ground truth annotations from the generator.\n",
    "\n",
    "    The result is a list of lists such that the size is:\n",
    "        all_annotations[num_images][num_classes] = annotations[num_class_annotations, 5]\n",
    "\n",
    "    Args:\n",
    "        generator: The generator used to retrieve ground truth annotations.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists containing the annotations for each image in the generator.\n",
    "\n",
    "    \"\"\"\n",
    "    all_annotations = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "\n",
    "    for i in progressbar.progressbar(range(generator.size()), prefix='Parsing annotations: '):\n",
    "        # load the annotations\n",
    "        annotations = generator.load_annotations(i)\n",
    "\n",
    "        # copy detections to all_annotations\n",
    "        for label in range(generator.num_classes()):\n",
    "            if not generator.has_label(label):\n",
    "                continue\n",
    "\n",
    "            all_annotations[i][label] = annotations['bboxes'][annotations['labels'] == label, :].copy()\n",
    "\n",
    "    return all_annotations\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "        generator,\n",
    "        model,\n",
    "        iou_threshold=0.5,\n",
    "        score_threshold=0.01,\n",
    "        max_detections=5,\n",
    "        visualize=False,\n",
    "        epoch=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a given dataset using a given model.\n",
    "\n",
    "    Args:\n",
    "        generator: The generator that represents the dataset to evaluate.\n",
    "        model: The model to evaluate.\n",
    "        iou_threshold: The threshold used to consider when a detection is positive or negative.\n",
    "        score_threshold: The score confidence threshold to use for detections.\n",
    "        max_detections: The maximum number of detections to use per image.\n",
    "        visualize: Show the visualized detections or not.\n",
    "\n",
    "    Returns:\n",
    "        A dict mapping class names to mAP scores.\n",
    "\n",
    "    \"\"\"\n",
    "    # gather all detections and annotations\n",
    "    all_detections= _get_detections(generator, model, score_threshold=score_threshold, max_detections=max_detections,\n",
    "                                     visualize=visualize)\n",
    "    all_annotations = _get_annotations(generator)\n",
    "    average_precisions = {}\n",
    "    num_tp = 0\n",
    "    num_fp = 0\n",
    "\n",
    "    \n",
    "    # process detections and annotations\n",
    "    for label in range(generator.num_classes()):\n",
    "        if not generator.has_label(label):\n",
    "            continue\n",
    "\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives = np.zeros((0,))\n",
    "        scores = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            detections = all_detections[i][label]\n",
    "            annotations = all_annotations[i][label]\n",
    "            num_annotations += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "\n",
    "            for d in detections:\n",
    "                scores = np.append(scores, d[4])\n",
    "\n",
    "                if annotations.shape[0] == 0:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives = np.append(true_positives, 0)\n",
    "                    continue\n",
    "                overlaps = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                max_overlap = overlaps[0, assigned_annotation]\n",
    "\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives = np.append(true_positives, 1)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives = np.append(true_positives, 0)\n",
    "\n",
    "        # no annotations -> AP for this class is 0 (is this correct?)\n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0, 0\n",
    "            continue\n",
    "\n",
    "        # sort by score\n",
    "        indices = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives = true_positives[indices]\n",
    "\n",
    "        # compute false positives and true positives\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives = np.cumsum(true_positives)\n",
    "\n",
    "        if false_positives.shape[0] == 0:\n",
    "            num_fp += 0\n",
    "        else:\n",
    "            num_fp += false_positives[-1]\n",
    "        if true_positives.shape[0] == 0:\n",
    "            num_tp += 0\n",
    "        else:\n",
    "            num_tp += true_positives[-1]\n",
    "\n",
    "        # compute recall and precision\n",
    "        recall = true_positives / num_annotations\n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "        # compute average precision\n",
    "        average_precision = _compute_ap(recall, precision)\n",
    "        average_precisions[label] = average_precision, num_annotations\n",
    "    print('num_fp={}, num_tp={}'.format(num_fp, num_tp))\n",
    "\n",
    "    return average_precisions,recall,precision,iou_threshold,max_detections\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from generators.pascal import PascalVocGenerator\n",
    "    from model import efficientdet\n",
    "    import os\n",
    "    f=open(\"/ssd3/u1/NBI_NET/EfficientDet-master/bbox.txt\",\"w\")\n",
    "    f.close()\n",
    "\n",
    "    max_detections=100\n",
    "    phi = 1\n",
    "    weighted_bifpn = False\n",
    "    common_args = {\n",
    "        'batch_size': 1,\n",
    "        'phi': phi,\n",
    "    }\n",
    "    test_generator = PascalVocGenerator(\n",
    "        'datasets/VOCdevkit/VOC2007',\n",
    "        'test',\n",
    "        shuffle_groups=False,\n",
    "        skip_truncated=False,\n",
    "        skip_difficult=True,\n",
    "        **common_args\n",
    "    )\n",
    "    model_path = 'checkpoints/fpi1-test01_R_ann4/pascal_04_0.4206_1.0106_879.h5'#fpi1_ann4\n",
    "#     model_path = 'checkpoints/fpi1-test02_R_ann2/pascal_02_0.0912_0.0791.h5'#fpi1_ann2\n",
    "#     model_path = 'checkpoints/fpi3-test11_R_ann4/best_pascal_06_0.3386_0.7871_642.h5'#fpi3_ann4\n",
    "#     model_path = 'checkpoints/fpi3-test12_R_ann2/pascal_03_0.0493_0.9485.h5'#fpi3_ann2\n",
    "    input_shape = (test_generator.image_size, test_generator.image_size)\n",
    "    anchors = test_generator.anchors\n",
    "    num_classes = test_generator.num_classes()\n",
    "    \n",
    "    model, prediction_model = efficientdet(phi=phi, num_classes=num_classes, weighted_bifpn=weighted_bifpn)\n",
    "    \n",
    "    prediction_model.load_weights(model_path, by_name=True)\n",
    "    tStart = time.time()\n",
    "    average_precisions,recall,precison,iou_threshold,max_detections= evaluate(test_generator, prediction_model,visualize=False)\n",
    "    tEnd = time.time()\n",
    "    print(\"It cost %f sec\" % (tEnd - tStart))\n",
    "    # compute per class average precision\n",
    "    total_instances = []\n",
    "    precisions = []\n",
    "    for label, (average_precision, num_annotations) in average_precisions.items():\n",
    "        print('{:.0f} instances of class'.format(num_annotations), test_generator.label_to_name(label),\n",
    "              'with average precision: {:.4f}'.format(average_precision))\n",
    "        total_instances.append(num_annotations)\n",
    "        precisions.append(average_precision)\n",
    "    mean_ap = sum(precisions) / sum(x > 0 for x in total_instances)\n",
    "    print(\"Max detections: \",max_detections)\n",
    "    print(\"IOU Threshold: \",iou_threshold)\n",
    "    print('mAP: {:.4f}'.format(mean_ap))\n",
    "    print('recall: ',np.mean(recall))\n",
    "    print('precison: ',np.mean(precison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
